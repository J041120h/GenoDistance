================================================================================
H5AD MEMORY INSPECTION REPORT
File: /dcl01/hongkai/data/data/hjiang/Data/count_data.h5ad
File size on disk: 1.96 GB
================================================================================

Loading file...

Basic Information:
  Number of observations (cells): 898,489
  Number of variables (genes): 14,881

----------------------------------------
1. MAIN DATA MATRIX (X)
----------------------------------------
  Memory usage: 11.97 GB
  Type: <class 'scipy.sparse._csc.csc_matrix'>
  Data type: float64
  Shape: (898489, 14881)

----------------------------------------
2. LAYERS
----------------------------------------
  No layers found

----------------------------------------
3. OBSERVATIONS METADATA (obs)
----------------------------------------
  Total memory: 106.27 MB
  Number of columns: 4

  Top memory-consuming columns:
    'sample': 103.69 MB (dtype: category)
    'celltype': 102.79 MB (dtype: category)
    'old': 102.79 MB (dtype: category)
    'large': 102.79 MB (dtype: category)

----------------------------------------
4. VARIABLES METADATA (var)
----------------------------------------
  Total memory: 1.39 MB
  Number of columns: 0

----------------------------------------
5. DIMENSIONAL REDUCTIONS (obsm)
----------------------------------------
  No dimensional reductions found

----------------------------------------
6. VARIABLE EMBEDDINGS (varm)
----------------------------------------
  No variable embeddings found

----------------------------------------
7. UNSTRUCTURED DATA (uns)
----------------------------------------
  Total memory: 55.00 B

  Top memory-consuming items:
    'X_name': 55.00 B (str)

----------------------------------------
8. PAIRWISE MATRICES
----------------------------------------
  No observation pairwise matrices found
  No variable pairwise matrices found

================================================================================
MEMORY SUMMARY
================================================================================

Total estimated memory in RAM: 12.07 GB

Breakdown by component:
  X         :     11.97 GB ( 99.1%)
  obs       :    106.27 MB (  0.9%)
  var       :      1.39 MB (  0.0%)
  uns       :      55.00 B (  0.0%)

================================================================================
OPTIMIZATION RECOMMENDATIONS
================================================================================

1. Convert main matrix X from float64 to float32 (save ~5.98 GB)

================================================================================
END OF REPORT
================================================================================

================================================================================
OPTIMIZATION CODE SUGGESTIONS
================================================================================

# Here's example code to optimize your h5ad file:

import scanpy as sc
import numpy as np
from scipy.sparse import csr_matrix

# Load the original file
adata = sc.read_h5ad('/dcl01/hongkai/data/data/hjiang/Data/count_data.h5ad')

# 1. Convert main matrix to float32 if using float64
if hasattr(adata.X, 'dtype') and adata.X.dtype == np.float64:
    if issparse(adata.X):
        adata.X = adata.X.astype(np.float32)
    else:
        adata.X = adata.X.astype(np.float32)
    print("Converted X to float32")

# 2. Convert dense matrix to sparse if mostly zeros
if isinstance(adata.X, np.ndarray):
    zero_fraction = np.sum(adata.X == 0) / adata.X.size
    if zero_fraction > 0.5:
        adata.X = csr_matrix(adata.X)
        print(f"Converted X to sparse ({zero_fraction:.1%} zeros)")

# 3. Optimize layers
for layer_name in list(adata.layers.keys()):
    layer = adata.layers[layer_name]
    
    # Convert to float32
    if hasattr(layer, 'dtype') and layer.dtype == np.float64:
        adata.layers[layer_name] = layer.astype(np.float32)
    
    # Convert to sparse if beneficial
    if isinstance(layer, np.ndarray):
        zero_fraction = np.sum(layer == 0) / layer.size
        if zero_fraction > 0.5:
            adata.layers[layer_name] = csr_matrix(layer)

# 4. Optimize embeddings
for key in adata.obsm.keys():
    if hasattr(adata.obsm[key], 'dtype') and adata.obsm[key].dtype == np.float64:
        adata.obsm[key] = adata.obsm[key].astype(np.float32)

# 5. Convert string columns to categorical
for col in adata.obs.columns:
    if adata.obs[col].dtype == 'object':
        unique_ratio = len(adata.obs[col].unique()) / len(adata.obs[col])
        if unique_ratio < 0.5:  # Less than 50% unique values
            adata.obs[col] = adata.obs[col].astype('category')

for col in adata.var.columns:
    if adata.var[col].dtype == 'object':
        unique_ratio = len(adata.var[col].unique()) / len(adata.var[col])
        if unique_ratio < 0.5:
            adata.var[col] = adata.var[col].astype('category')

# 6. Remove unnecessary data from uns
# Review what's in adata.uns and remove unneeded items
# Example: del adata.uns['some_large_unnecessary_key']

# 7. Save optimized file
output_path = '/dcl01/hongkai/data/data/hjiang/Data/count_data.h5ad'.replace('.h5ad', '_optimized.h5ad')
adata.write_h5ad(output_path, compression='gzip')
print(f"Saved optimized file to: {output_path}")

